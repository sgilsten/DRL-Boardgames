{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Learning to Win: Deep Reinforcement Learning (DRL) in Board Games**\n",
        "By Sophie Gilsten, Zihan Hei, Elizabeth Santana Lugo, and Emily Goldstein"
      ],
      "metadata": {
        "id": "gJ5THA0tkDb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction:**\n"
      ],
      "metadata": {
        "id": "SNMlAoD3tPJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deep Reinforcement Learning**"
      ],
      "metadata": {
        "id": "usrZQH0uHc-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement learning (RL) is a type of deep learning framework in which an agent learns to make decisions through trial and error, receiving feedback in the form of rewards. Unlike supervised learning, RL does not rely on labeled examples; instead, it follows the cycle of the agent taking an action, analyzing its environment, either receiving a reward or not, interpreting that, and then updating its strategy according to the Bellman equation, which calculates long-term rewards from taking optimal actions (Pacific Northwest National Library, 2021). Deep reinforcement learning is the process of extending this framework by using neural networks to enable agents to learn in environments with larger and more complex states."
      ],
      "metadata": {
        "id": "qnK3loLYaxKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Literature Review**"
      ],
      "metadata": {
        "id": "QOrOvtjUe-91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recent work in DRL emphasizes its growing impact across both technical and societal domains. Merchán (2023) highlights how DRL has quickly expanded beyond academic research into applied industries like robotics, finance, and healthcare, where models learn directly from experience rather than relying on specific data.\n",
        "\n",
        "\n",
        "Other research stresses the need for integrating human feedback into these systems to reduce misinformation, support safer outcomes, and improve alignment between AI systems and human objectives (Liu, 2023).\n",
        "\n",
        "\n",
        "Like most AI systems, the question of human oversight is an increasingly important concern as DRL becomes more widespread. Broader surveys, such as Whittlestone et al. (2021), show the rapid progression of DRL as both an opportunity and a governance challenge. Citing that while DRL drastically improves learning efficiency, its deployment requires careful consideration of transparency, oversight, and ethical risk.\n",
        "\n",
        "Within game-focused DRL research, prior studies consistently use self-play, reward-driven learning, and relatively small feed-forward networks to train agents in games like Tic-Tac-Toe and Connect Four. These works demonstrate that agents can reach strong or optimal play using only environmental feedback. Our project aligns with this body of research: rather than using external datasets or human demonstrations, we follow the established pattern of training agents through repeated self-play, implementing our own models and reward structures to observe how closely our results replicate or diverge from those reported in the literature.\n"
      ],
      "metadata": {
        "id": "d1jOpgU1fDfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementation**"
      ],
      "metadata": {
        "id": "PT17yns4bZKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Board games are an ideal testing environment for DRL because they are structured and straightforward. The rules are simple, the action spaces are well defined, and outcomes can be evaluated without ambiguity. This makes it easier to observe whether an agent is truly learning meaningful strategies rather than relying on chance. Their turn-based nature and clear win/loss outcomes align well with the reward-driven structure of reinforcement learning, making them natural candidates for experimentation."
      ],
      "metadata": {
        "id": "VwdPRLoAbkCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Our Motivation**\n",
        "\n"
      ],
      "metadata": {
        "id": "aQdZ_WDddOBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main motivation behind our project is to understand how DRL models learn strategic behavior through self-play. Starting with Tic-Tac-Toe allows us to test our implementation in a simple environment where learning can be monitored closely. Scaling to Connect Four offers an opportunity to examine whether the same training approach can handle a larger state space and more complex decision-making. We were also interested in comparing our models to external sources to see whether our architectures and training procedures could replicate similar patterns."
      ],
      "metadata": {
        "id": "6SeFtjRHdWVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Research Questions**\n",
        "\n"
      ],
      "metadata": {
        "id": "oNhOCgwpixH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is guided by several central questions:\n",
        "\n",
        "1. Can a deep Q-learning agent learn optimal play in Tic-Tac-Toe through self-play?\n",
        "\n",
        "\n",
        "2. Can a similar reinforcement learning approach scale effectively to Connect Four, where the state and action spaces are significantly larger?\n",
        "\n",
        "\n",
        "3. How do our results compare to those in the literature, particularly in terms of improvement over a random baseline and stability of learning?\n",
        "\n",
        "\n",
        "4. What challenges emerge when agents are trained without human-designed features, and how do these limitations affect training outcomes?"
      ],
      "metadata": {
        "id": "N4dHyZDDi0RR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data:**"
      ],
      "metadata": {
        "id": "G6PZyUJ9t1k7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our project did not use a dataset, because the only input necessary for training were the games themselves. We wrote our own code for both Tic-Tac-Toe and Connect Four. Our code built the board, assigned each player a letter or color, checked for available moves, and monitored the game for a win or tie."
      ],
      "metadata": {
        "id": "PdjId4tI-Uct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis:**\n"
      ],
      "metadata": {
        "id": "SQZSpk7oO5JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dDsGyF9iscgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tic-Tac-Toe Code**"
      ],
      "metadata": {
        "id": "c3IzDJqIwysV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step was coding the game itself. We made sure that our code included every necessary factor for a smooth game, such as making illegal moves impossible and checking for a win in every possible direction. We also included code that allowed us to play against the model in real time to make sure that our game worked. Both the game code and the play cell are included at the very end of this section so that the model is initiated first."
      ],
      "metadata": {
        "id": "bfcYy8emEFwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we confirmed that our Tic-Tac-Toe game worked, it was time to implement our model. We had to rewrite the Tic-Tac-Toe board in a way that made sense to a neural network."
      ],
      "metadata": {
        "id": "PKzlf7hVF8ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the board to a flat list of numbers: X is 1, O is -1, empty is 0\n",
        "\n",
        "def board_to_tensor(board):\n",
        "  flat = []\n",
        "\n",
        "  for row in board:\n",
        "    for cell in row:\n",
        "      if cell == \"X\":\n",
        "        flat.append(1)\n",
        "      elif cell == \"O\":\n",
        "        flat.append(-1)\n",
        "      else:\n",
        "        flat.append(0)\n",
        "  return torch.tensor(flat, dtype=torch.float32)\n",
        "\n",
        "# return a list of available positions (r,c)\n",
        "def available_moves(board):\n",
        "  moves = []\n",
        "\n",
        "  for i in range(3):\n",
        "    for j in range(3):\n",
        "      if board[i][j] == \" \":\n",
        "        moves.append((i,j))\n",
        "  return moves\n",
        "\n",
        "# convert 2D (r,c) to 0-8 action\n",
        "def flatten_action(r, c):\n",
        "  return r * 3 + c\n",
        "\n",
        "# convert 0-8 action to 2D (r,c)\n",
        "def unflatten_action(a):\n",
        "  return divmod(a, 3)\n"
      ],
      "metadata": {
        "id": "jLckoVBXsjOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we pulled the `check_win` function out of our initial code so we could train our model with it."
      ],
      "metadata": {
        "id": "n10PR56gI7J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check for win\n",
        "def check_win(board, player):\n",
        "    board = np.array(board)\n",
        "\n",
        "    # rows\n",
        "    for row in board:\n",
        "        if (row == player).sum() == 3:\n",
        "            return True\n",
        "    # columns\n",
        "    for c in range(3):\n",
        "        if board[0][c] == player and board[1][c] == player and board[2][c] == player:\n",
        "            return True\n",
        "    # diagonals\n",
        "    if board[0][0] == player and board[1][1] == player and board[2][2] == player:\n",
        "        return True\n",
        "    if board[0][2] == player and board[1][1] == player and board[2][0] == player:\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "pjUKLRgWwgnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coding the neural network itself was the next step. We used code from Nested Software (2019) as guidance. Our model had two inner layers, each with a linear component and a ReLU component. The input consisted of the nine board positions, and the output returned information about what was in each of the nine positions.\n",
        "\n",
        "Our original model only had one hidden layer. We added an additional layer for multiple reasons, as detailed below.\n",
        "\n",
        "**Network Architecture Enhancement**\n",
        "\n",
        "Original Architecture:\n",
        "\n",
        "* Input (9) -> Hidden (32) -> Output (9)\n",
        "\n",
        "* Parameters: 9×32 + 32×9 = 576 parameters\n",
        "\n",
        "Enhanced Architecture:\n",
        "\n",
        "* Input (9) -> Hidden1 (64) -> Hidden2 (32) -> Output (9)\n",
        "* Parameters: 9×64 + 64×32 + 32×9 = 2,912 parameters\n",
        "\n",
        "The additional hidden layer provides:\n",
        "\n",
        "* 5× increase in model capacity\n",
        "* Better feature representation for complex board patterns\n",
        "* Improved ability to learn both offensive and defensive strategies"
      ],
      "metadata": {
        "id": "QRbMNPBlJh7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# neural network with one hidden layer, 9 inputs, 9 outputs (one score for each possible move)\n",
        "# add more layer\n",
        "class TicTacToeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(9, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 32),\n",
        "        nn.ReLU(),      # new hidden layer\n",
        "        nn.Linear(32, 9)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.layers(x)\n",
        "\n",
        "net = TicTacToeNet()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "-fAiWVM2wj9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We had to teach the model how to choose a move. First it learned how to identify legal moves on the board. This ensure the Agent only picks legal moves, it creates a mask that blocks already filled spaces and converts the predictions into probabilities using softmax. Then, it used an epsilon-greedy strategy, where it could either trust the network or make a random move to explore more options. After choosing a move, it places an \"O\" on the board and returns the log-probability of that choice. This way the training algorithm uses it to reward good moves and punish bad ones, helping the network improve over time."
      ],
      "metadata": {
        "id": "lJ8S3lh_xN5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reinforcement learning computer move\n",
        "# to choose a move, the network output is turned into a probability distribution\n",
        "def rl_computer_move(board, epsilon=0.0):\n",
        "  state = board_to_tensor(board)         # convert current board into a flat tensor\n",
        "  logits = net(state)              # get the network's output logits\n",
        "  logits = torch.clamp(logits, -20, 20)  # prevents very large/small values\n",
        "\n",
        "# create a mask to mark legal moves (1 for empty, 0 for taken)\n",
        "# the agent selects a legal move as a sample based on these prob\n",
        "  mask = torch.zeros(9)\n",
        "  for r,c in available_moves(board):\n",
        "    mask[flatten_action(r,c)] = 1\n",
        "\n",
        "  # Epsilon-greedy: sometimes explore, sometimes exploit\n",
        "  if random.random() < epsilon:\n",
        "    # Explore: random legal move\n",
        "    legal_indices = [flatten_action(r, c) for r, c in available_moves(board)]\n",
        "    action_index = random.choice(legal_indices)\n",
        "    probs = torch.zeros(9)\n",
        "    probs[action_index] = 1.0\n",
        "  else:\n",
        "    # apply mask (illegal moves have 0 prob)\n",
        "    # Exploit: use network predictions\n",
        "    probs = torch.softmax(logits, dim=0)\n",
        "    probs = probs * mask\n",
        "    total = probs.sum()     # sum of legal move prob\n",
        "\n",
        "  # if sum is 0, pick random legal move\n",
        "  if total.item() == 0:\n",
        "    legal_indices = [flatten_action(r, c) for r, c in available_moves(board)]\n",
        "    action_index = random.choice(legal_indices)\n",
        "    probs = torch.zeros(9)\n",
        "    probs[action_index] = 1.0\n",
        "  else:\n",
        "    probs = probs / total  # otherwise, normalize prob so they sum to 1\n",
        "\n",
        "# choose action\n",
        "  dist = torch.distributions.Categorical(probs)   # creates a probability for choosing each move\n",
        "  action = dist.sample()               # picks one action from the distribution\n",
        "  r,c = unflatten_action(action.item())       # turns the pytorch tensor into integer and converts it into a board position\n",
        "\n",
        "  board[r][c] = \"O\"\n",
        "  return dist.log_prob(action)  # return the log-probability of the chosen move\n",
        "                  # if the agent wins, we reward moves with high log_prob\n",
        "                  # if it loses, we punish those moves"
      ],
      "metadata": {
        "id": "k9qyyFJVwmx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented the `minimax` algorithm as a backup strategy to try and ensure perfect play. It works by looking ahead at all possible moves and their outcomes: wins score +10, losses score -10, and ties score 0. The algorithm simulates every possible game situation, tries every move, then assumes the opponent will make the best response and checks all possibilities until the game ends.\n",
        "\n",
        "We combined this with our neural network. The network learns and makes most decisions, but minimax captures key steps that the neural network might miss and prevents basic failures."
      ],
      "metadata": {
        "id": "NAXbo2veyTTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimax algorithm for perfect play (backup strategy)\n",
        "def minimax(board, is_maximizing, player=\"O\", opponent=\"X\"):\n",
        "    # Check terminal states\n",
        "    if check_win(board, player):\n",
        "        return 10\n",
        "    if check_win(board, opponent):\n",
        "        return -10\n",
        "    if not available_moves(board):\n",
        "        return 0\n",
        "\n",
        "    if is_maximizing:\n",
        "        best_score = -float('inf')\n",
        "        for r, c in available_moves(board):\n",
        "            board[r][c] = player\n",
        "            score = minimax(board, False, player, opponent)\n",
        "            board[r][c] = \" \"\n",
        "            best_score = max(score, best_score)\n",
        "        return best_score\n",
        "    else:\n",
        "        best_score = float('inf')\n",
        "        for r, c in available_moves(board):\n",
        "            board[r][c] = opponent\n",
        "            score = minimax(board, True, player, opponent)\n",
        "            board[r][c] = \" \"\n",
        "            best_score = min(score, best_score)\n",
        "        return best_score"
      ],
      "metadata": {
        "id": "nqfMSnSrwsMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wrote a `find_best_move_minimax()` function that identifies the best move by scoring each option with minimax. It tries placing the player's marker in each empty spot, calls minimax to evaluate that move, then undoes it and compares scores to find the best choice. Finally, it returns the specific board position (row, column) that guarantees the best possible outcome against perfect play."
      ],
      "metadata": {
        "id": "sfvsUR9hyn7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_move_minimax(board, player=\"O\"):\n",
        "    best_score = -float('inf')\n",
        "    best_move = None\n",
        "    opponent = \"X\" if player == \"O\" else \"O\"\n",
        "\n",
        "    for r, c in available_moves(board):\n",
        "        board[r][c] = player\n",
        "        score = minimax(board, False, player, opponent)\n",
        "        board[r][c] = \" \"\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_move = (r, c)\n",
        "\n",
        "    return best_move"
      ],
      "metadata": {
        "id": "NvCt43PPw6jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `rl_computer_move_safe()` function adds safety to the agent by checking two things first: can the opponent win next turn (if yes, block them), and can we win right now (if yes, take it). The agent will only use the neural network to make decisions when neither of these two conditions exists, thereby avoiding missing obvious winning opportunities or making mistakes."
      ],
      "metadata": {
        "id": "ZUABmjGpyqbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rl_computer_move_safe(board, use_minimax_backup=True):\n",
        "  if use_minimax_backup:\n",
        "        # Check if opponent can win next turn - Block It\n",
        "        for r, c in available_moves(board):\n",
        "            board[r][c] = \"X\"\n",
        "            if check_win(board, \"X\"):\n",
        "                board[r][c] = \"O\"  # Block the win\n",
        "                return None\n",
        "            board[r][c] = \" \"\n",
        "\n",
        "        # Check if we can win this turn\n",
        "        for r, c in available_moves(board):\n",
        "            board[r][c] = \"O\"\n",
        "            if check_win(board, \"O\"):\n",
        "                return None  # Already placed the winning move\n",
        "            board[r][c] = \" \"\n",
        "\n",
        "    # Otherwise use neural network\n",
        "  return rl_computer_move(board, epsilon=0.0)"
      ],
      "metadata": {
        "id": "S5RikGgGw8tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step was to implement rewards and punishments. If the agent won a game, it got a +1 reward. If it lost a game, it got a -1 punishment. If there was a tie, the agent got a reward of 0. This is the reinforcement portion of our Deep Reinforcement Learning model."
      ],
      "metadata": {
        "id": "KbfeXgB-MiZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# play one game and train\n",
        "def train_one_game_vs_random():\n",
        "  board = [[\" \" for _ in range(3)] for _ in range(3)]\n",
        "  player1 = \"Human\"\n",
        "  player2 = \"RL_Agent\"\n",
        "  current_player = player1\n",
        "\n",
        "  log_probs = []   # store chosen moves of RL (for learning)\n",
        "  final_reward = 0    # final reward (+1 win, -1 lose, 0 tie)\n",
        "\n",
        "  while True:\n",
        "# human move (random for simulation, could be input)\n",
        "    if current_player == player1:\n",
        "      empty_moves = available_moves(board)   # get list of empty spots\n",
        "      if not empty_moves:\n",
        "        final_reward = 0  # tie\n",
        "        break\n",
        "\n",
        "      # pick a ramdon empty spot\n",
        "      r,c = random.choice(empty_moves)\n",
        "      board[r][c] = \"X\"\n",
        "\n",
        "    # RL agent move\n",
        "    else:\n",
        "      log_prob = rl_computer_move(board, epsilon=0.0)  # agent picks a move\n",
        "      if log_prob is not None:\n",
        "        log_probs.append(log_prob)\n",
        "\n",
        "    # check if anyone wins\n",
        "    if check_win(board, \"X\"):\n",
        "      final_reward = -1  # RL loses\n",
        "      break\n",
        "    elif check_win(board, \"O\"):\n",
        "      final_reward = +1   # RL wins\n",
        "      break\n",
        "\n",
        "    # check tie\n",
        "    if all(board[i][j] != \" \" for i in range(3) for j in range(3)):\n",
        "      final_reward = 0\n",
        "      break\n",
        "\n",
        "    # switch players\n",
        "    current_player = player2 if current_player == player1 else player1\n",
        "\n",
        "# train the RL agent (reinforce)\n",
        "  if log_probs:    # RL must make at least one move\n",
        "    loss = -torch.stack(log_probs).sum() * final_reward  # reinforce formula\n",
        "    optimizer.zero_grad()  # reset gradients\n",
        "    loss.backward()     # compute gradients\n",
        "    torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)  # gradient clipping\n",
        "    optimizer.step()     # update neural network\n",
        "\n",
        "  return final_reward\n",
        "\n",
        "# RL agent wins: reward = +1 (the agent successfully placed Os and made the human (or random opponent) lose)\n",
        "# RL agent loses: reward = -1 (the agent made bad moves, and the opponent (X) won)\n",
        "# Tie game: reward = 0 (neither player won; the board is full)"
      ],
      "metadata": {
        "id": "38ILYYSTw_DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initially chose to train the model through self-play. The same neural network controls both X and O players, but when playing as O, it flips the board perspective so O always sees itself as the attacking player (this helps the network learn both positions using the same weights). Each player's moves and their probabilities are recorded throughout the game. The only difference in the reward system is that when there is a tie, each player gets a small reward (+0.5)."
      ],
      "metadata": {
        "id": "HHgBOd1O0Gm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_game_selfplay():\n",
        "  board = [[\" \" for _ in range(3)] for _ in range(3)]\n",
        "\n",
        "  log_probs_player1 = []  # X player (first agent)\n",
        "  log_probs_player2 = []  # O player (second agent)\n",
        "  current_player = \"X\"\n",
        "\n",
        "  while True:\n",
        "    if current_player == \"X\":       # first agent plays as X\n",
        "      state = board_to_tensor(board)\n",
        "      logits = net(state)\n",
        "      logits = torch.clamp(logits, -20, 20)\n",
        "      probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "      mask = torch.zeros(9)\n",
        "      for r,c in available_moves(board):\n",
        "        mask[flatten_action(r,c)] = 1\n",
        "\n",
        "      probs = probs * mask\n",
        "      total = probs.sum()\n",
        "\n",
        "      if total.item() == 0:\n",
        "        legal_indices = [flatten_action(r, c) for r, c in available_moves(board)]\n",
        "        action_index = random.choice(legal_indices)\n",
        "        probs = torch.zeros(9)\n",
        "        probs[action_index] = 1.0\n",
        "      else:\n",
        "        probs = probs / total\n",
        "\n",
        "      dist = torch.distributions.Categorical(probs)\n",
        "      action = dist.sample()\n",
        "      r,c = unflatten_action(action.item())\n",
        "      board[r][c] = \"X\"\n",
        "      log_probs_player1.append(dist.log_prob(action))\n",
        "\n",
        "    else:\n",
        "      # Second agent plays as O - need to FLIP the board perspective\n",
        "      # Convert board so O sees itself as the positive player\n",
        "      flipped_board = [[\" \" for _ in range(3)] for _ in range(3)]\n",
        "      for i in range(3):\n",
        "        for j in range(3):\n",
        "          if board[i][j] == \"O\":\n",
        "            flipped_board[i][j] = \"X\"  # O becomes X in flipped view\n",
        "          elif board[i][j] == \"X\":\n",
        "            flipped_board[i][j] = \"O\"  # X becomes O in flipped view\n",
        "          else:\n",
        "            flipped_board[i][j] = \" \"\n",
        "\n",
        "      state = board_to_tensor(flipped_board)\n",
        "      logits = net(state)\n",
        "      logits = torch.clamp(logits, -20, 20)\n",
        "      probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "      mask = torch.zeros(9)\n",
        "      for r,c in available_moves(board):\n",
        "        mask[flatten_action(r,c)] = 1\n",
        "\n",
        "      probs = probs * mask\n",
        "      total = probs.sum()\n",
        "\n",
        "      if total.item() == 0:\n",
        "        legal_indices = [flatten_action(r, c) for r, c in available_moves(board)]\n",
        "        action_index = random.choice(legal_indices)\n",
        "        probs = torch.zeros(9)\n",
        "        probs[action_index] = 1.0\n",
        "      else:\n",
        "        probs = probs / total\n",
        "\n",
        "      dist = torch.distributions.Categorical(probs)\n",
        "      action = dist.sample()\n",
        "      r,c = unflatten_action(action.item())\n",
        "      board[r][c] = \"O\"\n",
        "      log_probs_player2.append(dist.log_prob(action))\n",
        "\n",
        "    # Check wins\n",
        "    if check_win(board, \"X\"):\n",
        "      # X wins: reward X, punish O\n",
        "      reward_X = 1\n",
        "      reward_O = -1\n",
        "      break\n",
        "\n",
        "    elif check_win(board, \"O\"):\n",
        "      # O wins: punish X, reward O\n",
        "      reward_X = 1\n",
        "      reward_O = -1\n",
        "      break\n",
        "\n",
        "    # Check tie\n",
        "    if all(board[i][j] != \" \" for i in range(3) for j in range(3)):\n",
        "      # Tie: small positive reward for both\n",
        "      reward_X = 0.5\n",
        "      reward_O = 0.5\n",
        "      break\n",
        "\n",
        "    # Switch players\n",
        "    current_player = \"O\" if current_player == \"X\" else \"X\"\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  total_loss = 0\n",
        "  if log_probs_player1:\n",
        "    loss_X = -torch.stack(log_probs_player1).sum() * reward_X\n",
        "    total_loss = total_loss + loss_X\n",
        "\n",
        "  if log_probs_player2:\n",
        "    loss_O = -torch.stack(log_probs_player2).sum() * reward_O\n",
        "    total_loss = total_loss + loss_O\n",
        "\n",
        "  if total_loss != 0:\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "  return reward_O"
      ],
      "metadata": {
        "id": "h5FuYWiaxC16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "C0xvBOVfxHDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Challenges Encountered**:\n",
        "\n",
        "**The Self-Play Training Challenge**: Initial attempts at self-play training resulted in imbalance, with the first player (X) winning 4,999 out of 5,000 games against the second player (O). We realized this was because when both players use the same neural network, they share the same perspective bias. The network learned to represent board states from X's viewpoint (X=+1, O=-1). This created an inherent asymmetry where:\n",
        "\n",
        "* X player views the board correctly from its trained perspective\n",
        "* O player views the board from an inverted perspective, causing strategic confusion\n",
        "\n",
        "**Mathematical Representation:**\n",
        "\n",
        "Board state for X: [+1, -1, 0, ...] (X sees itself as positive)\n",
        "\n",
        "Board state for O: [+1, -1, 0, ...] (O sees X as positive (WRONG!))"
      ],
      "metadata": {
        "id": "3OuNxEwa0Yx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Solutions**:\n",
        "**Four-Phase Process Learning**\n",
        "\n",
        "We developed a 4 phases training process to address the challenges identified:\n",
        "\n",
        "**Phase 1: Foundation Training as O (5,000 episodes)**\n",
        "\n",
        "The objective of our initial phase was for the model to learn basic *second-player strategies*. Its opponent was a random agent. After Phase 1 training, we expected our model to have a 60-70% win rate and develop blocking patterns."
      ],
      "metadata": {
        "id": "JFV40mem1cH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MIXED TRAINING: Start with random opponents, then add self-play\n",
        "set_seed(42)\n",
        "print(\"Phase 1: Training against random opponents...\")\n",
        "episodes_phase1 = 5000\n",
        "wins = losses = ties = 0\n",
        "for episode in range(episodes_phase1):\n",
        "    r = train_one_game_vs_random()\n",
        "    if r == 1: wins += 1\n",
        "    elif r == -1: losses += 1\n",
        "    else: ties += 1\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode {episode}, Wins={wins}, Losses={losses}, Ties={ties}\")\n",
        "\n",
        "print(f\"Phase 1 complete. Final - Wins={wins}, Losses={losses}, Ties={ties}\")"
      ],
      "metadata": {
        "id": "zutjwtxzxJAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8775f295-f344-4125-e925-2c956f7a0998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 1: Training against random opponents...\n",
            "Episode 0, Wins=1, Losses=0, Ties=0\n",
            "Episode 1000, Wins=609, Losses=328, Ties=64\n",
            "Episode 2000, Wins=1246, Losses=618, Ties=137\n",
            "Episode 3000, Wins=1869, Losses=937, Ties=195\n",
            "Episode 4000, Wins=2480, Losses=1238, Ties=283\n",
            "Phase 1 complete. Final - Wins=3103, Losses=1549, Ties=348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 2: First-Player Training as X (5,000 episodes)**\n",
        "\n",
        "The objective of Phase 2 was for the model to learn *first player* advantage strategies. Its opponent was still a random agent. The expected outcome of this phase was for our model to have a win rate of 70-80%, since it had the advantage of going first, and to develop aggressive opeingin moves."
      ],
      "metadata": {
        "id": "WQPFC7w63QOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add defensive training - agent plays as X sometimes\n",
        "def train_defensive_game():\n",
        "    # Agent plays as X (goes first), opponent is random\n",
        "    board = [[\" \" for _ in range(3)] for _ in range(3)]\n",
        "    log_probs = []\n",
        "    final_reward = 0\n",
        "\n",
        "    for move_count in range(9):\n",
        "        if move_count % 2 == 0:  # Agent plays as X (even moves)\n",
        "            state = board_to_tensor(board)\n",
        "            # Flip perspective: X is now 1, O is -1 (same as before but agent is X)\n",
        "            logits = net(state)\n",
        "            logits = torch.clamp(logits, -20, 20)\n",
        "            probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "            mask = torch.zeros(9)\n",
        "            for r,c in available_moves(board):\n",
        "                mask[flatten_action(r,c)] = 1\n",
        "\n",
        "            probs = probs * mask\n",
        "            total = probs.sum()\n",
        "\n",
        "            if total.item() == 0:\n",
        "                legal_indices = [flatten_action(r, c) for r, c in available_moves(board)]\n",
        "                action_index = random.choice(legal_indices)\n",
        "                probs = torch.zeros(9)\n",
        "                probs[action_index] = 1.0\n",
        "            else:\n",
        "                probs = probs / total\n",
        "\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            action = dist.sample()\n",
        "            r,c = unflatten_action(action.item())\n",
        "            board[r][c] = \"X\"\n",
        "            log_probs.append(dist.log_prob(action))\n",
        "\n",
        "            if check_win(board, \"X\"):\n",
        "                final_reward = +1\n",
        "                break\n",
        "            if not available_moves(board):\n",
        "                final_reward = 0\n",
        "                break\n",
        "        else:  # Random opponent plays as O\n",
        "            empty_moves = available_moves(board)\n",
        "            if not empty_moves:\n",
        "                final_reward = 0\n",
        "                break\n",
        "            r,c = random.choice(empty_moves)\n",
        "            board[r][c] = \"O\"\n",
        "\n",
        "            if check_win(board, \"O\"):\n",
        "                final_reward = -1\n",
        "                break\n",
        "\n",
        "    if log_probs:\n",
        "        loss = -torch.stack(log_probs).sum() * final_reward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    return final_reward"
      ],
      "metadata": {
        "id": "Mjnw_6kNxLay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Phase 2: Defensive training (learn both X and O positions)...\")\n",
        "episodes_phase2 = 5000\n",
        "wins = losses = ties = 0\n",
        "for episode in range(episodes_phase2):\n",
        "    r = train_defensive_game()\n",
        "    if r == 1: wins += 1\n",
        "    elif r == -1: losses += 1\n",
        "    else: ties += 1\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode {episode}, Wins={wins}, Losses={losses}, Ties={ties}\")\n",
        "\n",
        "print(f\"Phase 2 complete. Final - Wins={wins}, Losses={losses}, Ties={ties}\")"
      ],
      "metadata": {
        "id": "cmP601uqxO5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998f89e8-c880-465a-a0af-ac5f408d0815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 2: Defensive training (learn both X and O positions)...\n",
            "Episode 0, Wins=1, Losses=0, Ties=0\n",
            "Episode 1000, Wins=773, Losses=146, Ties=82\n",
            "Episode 2000, Wins=1554, Losses=292, Ties=155\n",
            "Episode 3000, Wins=2351, Losses=437, Ties=213\n",
            "Episode 4000, Wins=3156, Losses=577, Ties=268\n",
            "Phase 2 complete. Final - Wins=3958, Losses=709, Ties=333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 3: Balanced Mixed Training (5,000 episodes)**\n",
        "\n",
        "The objective of Phase 3 was to reinforce both positions equally by having the agent *alternate training* between X and O each epoch. The model played against itself. The expected outcome of this phase was for both positions to maintain a win rate greater than 65%."
      ],
      "metadata": {
        "id": "gjkkhoaB4dRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Phase 3: Balanced self-play (perfect defense)...\")\n",
        "episodes_phase3 = 5000\n",
        "x_wins = o_wins = ties = 0\n",
        "for episode in range(episodes_phase3):\n",
        "    r = train_one_game_selfplay()\n",
        "    if r == 1: o_wins += 1\n",
        "    elif r == -1: x_wins += 1\n",
        "    else: ties += 1\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode {episode}, X_wins={x_wins}, O_wins={o_wins}, Ties={ties}\")\n",
        "\n",
        "print(f\"Phase 3 complete. X_wins={x_wins}, O_wins={o_wins}, Ties={ties}\")"
      ],
      "metadata": {
        "id": "OUEBcdeixUfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437e7662-c82c-4e49-a7ed-d3b0b69ea4bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 3: Balanced self-play (perfect defense)...\n",
            "Episode 0, X_wins=1, O_wins=0, Ties=0\n",
            "Episode 1000, X_wins=826, O_wins=0, Ties=175\n",
            "Episode 2000, X_wins=1675, O_wins=0, Ties=326\n",
            "Episode 3000, X_wins=2514, O_wins=0, Ties=487\n",
            "Episode 4000, X_wins=3353, O_wins=0, Ties=648\n",
            "Phase 3 complete. X_wins=4175, O_wins=0, Ties=825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 4: Self-Play Refinement (5,000 episodes)**\n",
        "\n",
        "The objective of this phase was to have the agent play against itself, but this time, with *proper perspective handling*. The expected outcome of this phase was a high tie rate, especially since defensive strategies were enhanced by prior phases."
      ],
      "metadata": {
        "id": "8KRf9M635y5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Phase 4: Final refinement against random...\")\n",
        "episodes_phase4 = 5000\n",
        "wins = losses = ties = 0\n",
        "for episode in range(episodes_phase4):\n",
        "    # Alternate between playing as O and X\n",
        "    if episode % 2 == 0:\n",
        "        r = train_one_game_vs_random()\n",
        "    else:\n",
        "        r = train_defensive_game()\n",
        "\n",
        "    if r == 1: wins += 1\n",
        "    elif r == -1: losses += 1\n",
        "    else: ties += 1\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode {episode}, Wins={wins}, Losses={losses}, Ties={ties}\")\n",
        "\n",
        "print(f\"Phase 4 complete. Final - Wins={wins}, Losses={losses}, Ties={ties}\")"
      ],
      "metadata": {
        "id": "9lJswK4ZxVBY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a3a972-7d2d-4a7c-9037-000cfd174ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 4: Final refinement against random...\n",
            "Episode 0, Wins=1, Losses=0, Ties=0\n",
            "Episode 1000, Wins=662, Losses=249, Ties=90\n",
            "Episode 2000, Wins=1391, Losses=460, Ties=150\n",
            "Episode 3000, Wins=2130, Losses=655, Ties=216\n",
            "Episode 4000, Wins=2893, Losses=823, Ties=285\n",
            "Phase 4 complete. Final - Wins=3598, Losses=1040, Ties=362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluating Our Tic-Tac-Toe Model\n",
        "\n",
        "After training, we tested the strength of our model by having it play 1,000 games against a random agent. The results are plotted in the bar graph below."
      ],
      "metadata": {
        "id": "Np3DHTZT76GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Assessment - How likely the computer will win the game\n",
        "\n",
        "def evaluate_win_rate(num_games=1000, use_safe_mode=True):\n",
        "    wins = 0\n",
        "    ties = 0\n",
        "    losses = 0\n",
        "\n",
        "    for _ in range(num_games):\n",
        "\n",
        "        # Initialize empty board\n",
        "        board = [[\" \" for i in range(3)] for j in range(3)]\n",
        "        current_player = \"Human\"  # Random human opponent\n",
        "\n",
        "        while True:\n",
        "            if current_player == \"Human\":\n",
        "                empty_moves = available_moves(board)\n",
        "                if not empty_moves:\n",
        "                    # Tie\n",
        "                    ties += 1\n",
        "                    break\n",
        "                r, c = random.choice(empty_moves)\n",
        "                board[r][c] = \"X\"\n",
        "            else:\n",
        "              if use_safe_mode:\n",
        "                rl_computer_move_safe(board, use_minimax_backup=True)\n",
        "              else:\n",
        "                rl_computer_move(board, epsilon=0.0)\n",
        "\n",
        "            # Check win\n",
        "            if check_win(board, \"X\"):\n",
        "                losses += 1\n",
        "                break\n",
        "            elif check_win(board, \"O\"):\n",
        "                wins += 1\n",
        "                break\n",
        "\n",
        "            # Check tie\n",
        "            if all(board[i][j] != \" \" for i in range(3) for j in range(3)):\n",
        "                ties += 1\n",
        "                break\n",
        "\n",
        "            # Switch player\n",
        "            current_player = \"RL_Agent\" if current_player == \"Human\" else \"Human\"\n",
        "\n",
        "\n",
        "    # Compute rates\n",
        "    results = {\n",
        "        \"win\": wins / num_games,\n",
        "        \"loss\": losses / num_games,\n",
        "        \"tie\": ties / num_games\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    mode_str = \"WITH Minimax Backup\" if use_safe_mode else \"RL Only\"\n",
        "    print(f\"\\n{mode_str}\")\n",
        "    print(f\"Number of Games: {num_games}, Wins={wins}, Losses={losses}, Ties={ties}\")\n",
        "    print(f\"Win rate: {results['win']:.2%}, Loss rate: {results['loss']:.2%}, Tie rate: {results['tie']:.2%}\")\n",
        "\n",
        "    # Plot bar chart\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(results.keys(), results.values(), color=[\"green\", \"red\", \"gray\"])\n",
        "    plt.ylabel(\"Rate\")\n",
        "    plt.title(f\"RL Agent Performance over {num_games} Games\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "hptXiXPuxXQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.load_state_dict(torch.load(\"tictactoe_selfplay_v1.pth\"))\n",
        "net.eval()\n",
        "set_seed(42)\n",
        "evaluate_win_rate(num_games=1000, use_safe_mode=True)"
      ],
      "metadata": {
        "id": "tcydxr5ZxaYi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "d4310fe6-b48f-4cdd-b422-014f8541427f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WITH Minimax Backup\n",
            "Number of Games: 1000, Wins=624, Losses=36, Ties=340\n",
            "Win rate: 62.40%, Loss rate: 3.60%, Tie rate: 34.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'win': 0.624, 'loss': 0.036, 'tie': 0.34}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model wasn't as strong as we wanted, so we continued training using the mixed training approach. This way, our model could strengthen both defensive and offensive strategies."
      ],
      "metadata": {
        "id": "cSbEbhch8_cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue Training with mixed approach\n",
        "print(\"\\nContinued Training Phase: Mixed X/O training...\")\n",
        "episodes = 5000\n",
        "wins = losses = ties = 0\n",
        "for episode in range(episodes):\n",
        "    # Alternate between X and O training\n",
        "    if episode % 2 == 0:\n",
        "        r = train_one_game_vs_random()  # Train as O\n",
        "    else:\n",
        "        r = train_defensive_game()  # Train as X\n",
        "\n",
        "    if r == 1: wins += 1\n",
        "    elif r == -1: losses += 1\n",
        "    else: ties += 1\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode {episode}, Wins={wins}, Losses={losses}, Ties={ties}\")\n",
        "\n",
        "print(f\"\\nContinued training complete. Final - Wins={wins}, Losses={losses}, Ties={ties}\")"
      ],
      "metadata": {
        "id": "LboQTuO8xcV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec46711-245b-476a-bc23-34735ca0aa5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Continued Training Phase: Mixed X/O training...\n",
            "Episode 0, Wins=0, Losses=1, Ties=0\n",
            "Episode 1000, Wins=761, Losses=192, Ties=48\n",
            "Episode 2000, Wins=1511, Losses=389, Ties=101\n",
            "Episode 3000, Wins=2233, Losses=595, Ties=173\n",
            "Episode 4000, Wins=2941, Losses=812, Ties=248\n",
            "\n",
            "Continued training complete. Final - Wins=3639, Losses=1032, Ties=329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.load_state_dict(torch.load(\"tictactoe_selfplay_v2.pth\"))\n",
        "net.eval()\n",
        "set_seed(42)\n",
        "evaluate_win_rate(num_games=1000, use_safe_mode=True)"
      ],
      "metadata": {
        "id": "zAZTDQO8xg70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "94f4d830-72ad-4fea-eb2d-d5d750ed021e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WITH Minimax Backup\n",
            "Number of Games: 1000, Wins=645, Losses=13, Ties=342\n",
            "Win rate: 64.50%, Loss rate: 1.30%, Tie rate: 34.20%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'win': 0.645, 'loss': 0.013, 'tie': 0.342}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we evaluated our model by letting it play against itself. We expected both its offensive and defensive strategies to be well-developed, which should result in a very high number of ties."
      ],
      "metadata": {
        "id": "aJ3Oczb49VAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_self_play(num_games=1000):\n",
        "# Evaluate the neural network by letting it play against itself.\n",
        "# X and O both use the trained network.\n",
        "\n",
        "    x_wins = 0\n",
        "    o_wins = 0\n",
        "    ties = 0\n",
        "\n",
        "    for _ in range(num_games):\n",
        "        board = [[\" \" for _ in range(3)] for _ in range(3)]\n",
        "        current_player = \"X\"  # X always starts\n",
        "\n",
        "        while True:\n",
        "            if current_player == \"X\":\n",
        "                state = board_to_tensor(board)\n",
        "                logits = net(state)\n",
        "                logits = torch.clamp(logits, -20, 20)\n",
        "                probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "                mask = torch.zeros(9)\n",
        "                for r,c in available_moves(board):\n",
        "                    mask[flatten_action(r,c)] = 1\n",
        "                probs = probs * mask\n",
        "                total = probs.sum()\n",
        "                if total.item() == 0:\n",
        "                    legal_indices = [flatten_action(r, c) for r, c in available_moves(board)]\n",
        "                    action_index = random.choice(legal_indices)\n",
        "                    probs = torch.zeros(9)\n",
        "                    probs[action_index] = 1.0\n",
        "                else:\n",
        "                    probs = probs / total\n",
        "\n",
        "                dist = torch.distributions.Categorical(probs)\n",
        "                action = dist.sample()\n",
        "                r, c = unflatten_action(action.item())\n",
        "                board[r][c] = \"X\"\n",
        "\n",
        "            else:  # O's turn, flip board perspective\n",
        "                flipped_board = [[\" \" for _ in range(3)] for _ in range(3)]\n",
        "                for i in range(3):\n",
        "                    for j in range(3):\n",
        "                        if board[i][j] == \"O\":\n",
        "                            flipped_board[i][j] = \"X\"\n",
        "                        elif board[i][j] == \"X\":\n",
        "                            flipped_board[i][j] = \"O\"\n",
        "                        else:\n",
        "                            flipped_board[i][j] = \" \"\n",
        "                state = board_to_tensor(flipped_board)\n",
        "                logits = net(state)\n",
        "                logits = torch.clamp(logits, -20, 20)\n",
        "                probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "                mask = torch.zeros(9)\n",
        "                for r,c in available_moves(board):\n",
        "                    mask[flatten_action(r,c)] = 1\n",
        "                probs = probs * mask\n",
        "                total = probs.sum()\n",
        "                if total.item() == 0:\n",
        "                    legal_indices = [flatten_action(r, c) for r, c in available_moves(board)]\n",
        "                    action_index = random.choice(legal_indices)\n",
        "                    probs = torch.zeros(9)\n",
        "                    probs[action_index] = 1.0\n",
        "                else:\n",
        "                    probs = probs / total\n",
        "\n",
        "                dist = torch.distributions.Categorical(probs)\n",
        "                action = dist.sample()\n",
        "                r, c = unflatten_action(action.item())\n",
        "                board[r][c] = \"O\"\n",
        "\n",
        "            # Check win/tie\n",
        "            if check_win(board, \"X\"):\n",
        "                x_wins += 1\n",
        "                break\n",
        "            elif check_win(board, \"O\"):\n",
        "                o_wins += 1\n",
        "                break\n",
        "            elif not available_moves(board):\n",
        "                ties += 1\n",
        "                break\n",
        "\n",
        "            # Switch player\n",
        "            current_player = \"O\" if current_player == \"X\" else \"X\"\n",
        "\n",
        "    # Compute rates\n",
        "    results = {\n",
        "        \"X wins\": x_wins / num_games,\n",
        "        \"O wins\": o_wins / num_games,\n",
        "        \"Ties\": ties / num_games\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nSelf-play results over {num_games} games:\")\n",
        "    print(f\"  X wins: {x_wins}\")\n",
        "    print(f\"  O wins: {o_wins}\")\n",
        "    print(f\"  Ties:   {ties}\")\n",
        "    print(results)\n",
        "\n",
        "    # Plot bar chart\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(results.keys(), results.values(), color=[\"blue\", \"orange\", \"gray\"])\n",
        "    plt.ylabel(\"Rate\")\n",
        "    plt.title(f\"Self-Play Performance over {num_games} Games\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "gOSzgWpDxl0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.load_state_dict(torch.load(\"tictactoe_selfplay_v2.pth\"))\n",
        "net.eval()\n",
        "set_seed(42)\n",
        "evaluate_self_play(num_games=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "6tXoMEKQV_kc",
        "outputId": "5fa3226a-c398-4ff2-c40a-cdf2439527ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Self-play results over 1000 games:\n",
            "  X wins: 12\n",
            "  O wins: 214\n",
            "  Ties:   774\n",
            "{'X wins': 0.012, 'O wins': 0.214, 'Ties': 0.774}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X wins': 0.012, 'O wins': 0.214, 'Ties': 0.774}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes_self_play = 5000\n",
        "for episode in range(episodes_self_play):\n",
        "    train_one_game_selfplay()  # network plays both X and O\n",
        "\n",
        "evaluate_self_play(num_games=1000)"
      ],
      "metadata": {
        "id": "W2kptz4vxsA0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "ae9c1fd4-47f9-4923-9bc5-f39175ef9c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Self-play results over 1000 games:\n",
            "  X wins: 0\n",
            "  O wins: 0\n",
            "  Ties:   1000\n",
            "{'X wins': 0.0, 'O wins': 0.0, 'Ties': 1.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X wins': 0.0, 'O wins': 0.0, 'Ties': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the Tic-Tac-Toe game code and the cell to initiate real-time play."
      ],
      "metadata": {
        "id": "NWQCgcZHXWLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# makes game board\n",
        "def game_board(board):\n",
        "    print(\"-------------\")\n",
        "    for n in range(3):\n",
        "        print(\"|\", end=\" \")\n",
        "        for m in range(3):\n",
        "            print(board[n][m], end=\" | \")\n",
        "        print(\"\\n-------------\")\n",
        "\n",
        "#function for how the game runs\n",
        "def game_play():\n",
        "    #brings back in the board we created\n",
        "    board = [[\" \" for _ in range(3)] for _ in range(3)]\n",
        "\n",
        "    player_name = input(\"Enter your name: \")\n",
        "    current_player = player_name    #player always starts\n",
        "\n",
        "    while True:\n",
        "        game_board(board)\n",
        "\n",
        "        if current_player == player_name:\n",
        "            #what happens when I take my turn\n",
        "            while True:\n",
        "                try:\n",
        "                    row = int(input(\"Enter the row (1-3): \")) - 1 #b/c python is 0 indexed but i thought it would be clearer 1-3 not 0-2\n",
        "                    col = int(input(\"Enter the column (1-3): \")) - 1\n",
        "                except ValueError:\n",
        "                    print(\"You tricky bastard that's not a number\") #if the input is not numbers\n",
        "                    continue\n",
        "\n",
        "                if row not in {0,1,2} or col not in {0,1,2}:\n",
        "                    print(\"Ruh Roh that row/column doesn't exist please enter a number between 1 and 3.\") #if the input is not proper row/column\n",
        "                    continue\n",
        "\n",
        "                if board[row][col] == \" \":\n",
        "                    board[row][col] = \"X\" #if correct input put an X there\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"That spot is already taken :( try again\") #if correct input but not available\n",
        "\n",
        "        else:\n",
        "          # RL agent turn (uses trained model)\n",
        "          state = board_to_tensor(board)\n",
        "          logits = net(state)\n",
        "          probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "          mask = torch.zeros(9)\n",
        "          for r,c in available_moves(board):\n",
        "            mask[flatten_action(r,c)] = 1\n",
        "          probs = probs * mask\n",
        "          total = probs.sum()\n",
        "          if total.item() == 0:\n",
        "            # No legal moves pick random\n",
        "            r, c = random.choice(available_moves(board))\n",
        "          else:\n",
        "            probs = probs / total\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            action = dist.sample()\n",
        "            r, c = unflatten_action(action.item())\n",
        "          board[r][c] = \"O\"\n",
        "\n",
        "        #check wins based on earlier function\n",
        "        symbol = \"X\" if current_player == player_name else \"O\"\n",
        "        if check_win(board, symbol):\n",
        "            game_board(board)\n",
        "            print(f\"{current_player} wins!\")\n",
        "            break\n",
        "\n",
        "        #check if tie\n",
        "        if all(board[i][j] != \" \" for i in range(3) for j in range(3)):\n",
        "            game_board(board)\n",
        "            print(\"It's a tie!\")\n",
        "            break\n",
        "\n",
        "       #switches between the two players\n",
        "        current_player = player_name if current_player != player_name else \"RL Agent\""
      ],
      "metadata": {
        "id": "oXP6TkBbHSKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game_play()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pTrgMGiHUie",
        "outputId": "fbf21130-e8e0-4abd-c6d8-7c2a0ac954ef",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your name: Sophie\n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "Enter the row (1-3): 2\n",
            "Enter the column (1-3): 2\n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "-------------\n",
            "|   | O |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "Enter the row (1-3): 1\n",
            "Enter the column (1-3): 1\n",
            "-------------\n",
            "| X | O |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "-------------\n",
            "| X | O |   | \n",
            "-------------\n",
            "| O | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "Enter the row (1-3): 3\n",
            "Enter the column (1-3): 3\n",
            "-------------\n",
            "| X | O |   | \n",
            "-------------\n",
            "| O | X |   | \n",
            "-------------\n",
            "|   |   | X | \n",
            "-------------\n",
            "Sophie wins!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discussion and Explanation of Analysis**\n",
        "**Training Performance Evolution**\n",
        "\n",
        "The training results demonstrate clear learning progression across all phases:\n",
        "\n",
        "**Phase 1 Results (vs. Random, playing as O)**:\n",
        "\n",
        "* Wins: 3,103 (62.06%)\n",
        "* Losses: 1,549 (30.98%)\n",
        "* Ties: 348 (6.96%)\n",
        "\n",
        "The agent achieved a respectable 66.86% win rate against random play, shows effective pattern recognition for offensive positions. However, the 26% loss rate indicated defensive vulnerabilities that Phase 2 addressed.\n",
        "\n",
        "**Phase 2 Results (Defensive Training, playing as X)**:\n",
        "\n",
        "* Wins: 3,958, (79.16%)\n",
        "* Losses: 709 (14.18%)\n",
        "* Ties: 333 (6.66%)\n",
        "\n",
        "Defensive training improved the agent's ability to block opponent, reducing losses to under 18%. The increased tie rate (11.40%) suggests the agent learned more conservative, defensive strategies.\n",
        "\n",
        "**Phase 3 Results (Self-Play)**:\n",
        "\n",
        "* X wins: 4,175 (83.5%)\n",
        "* O wins: 0 (0%)\n",
        "* Ties: 825 (16.5%)\n",
        "\n",
        "The highly imbalance in self-play results reveals an important limitation that the agent developed a strong first move advantage but failed to learn optimal defensive play from the O position. This unbalance shows that perspective switching in self-play requires more advanced training approaches.\n",
        "\n",
        "**Phase 4 Results (Mixed Refinement)**:\n",
        "\n",
        "* Wins: 3,598 (71.96%)\n",
        "* Losses: 1,040 (20.8%)\n",
        "* Ties: 362 (7.24%)\n",
        "\n",
        "The final refinement phase successfully balanced offensive and defensive capabilities, achieving a 74% win rate with reduced losses compared to Phase 1. Overall the results may vary a little when running the different phases but should be around a 5% loss rate whereas the final evaluations are based on saved versions of our previous runs.\n",
        "\n",
        "**Final Evaluation with Safety Mechanisms**\n",
        "\n",
        "When evaluated against random opponents with minimax backup enabled (1,000 games):\n",
        "\n",
        "**Improved Model (additional 10,000 mixed episodes)**:\n",
        "\n",
        "* Wins: 3,639 (72.78%)\n",
        "* Losses: 1,032 (20.64%)\n",
        "* Ties: 329 (6.58%)\n",
        "\n",
        "The evaluation plot over 1000 games achieved a reduction in losses (from 20.64% to 1.3%) while maintaining similar win and tie rates. The loss rate of 1.3% represents near optimal performance, as even perfect tic-tac-toe play results in draws against competent opponents.\n",
        "\n",
        "**Self-Play Analysis: Perfect Defense**\n",
        "\n",
        "After an additional 5,000 episodes of pure self-play training, the agent achieved perfect defensive play:\n",
        "\n",
        "* X wins: 0 (0%)\n",
        "* O wins: 0 (0%)\n",
        "* Ties: 1,000 (100%)\n",
        "\n",
        "This result shows that the agent has converged to optimal defensive strategy, and recognizing that for a perfect play from both side in tic-tac-toe game need to results in a draw. The 100% tie rate indicates the agent successfully learned to block all winning opportunities."
      ],
      "metadata": {
        "id": "Xw7nB-uRfA6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Comparison with Literature**\n",
        "In the article *Teaching Agents to Play Tic-Tac-Toe Using Reinforcement Learning* by Kaneel Senevirathne (2021), the agent’s win rate increased rapidly during early training before plateauing once it discovered optimal strategies. Our learning curves exhibit the same shape: an initial unstable phase, followed by sharp improvement, and finally a high-rate convergence to near-perfect play. However, we developed a mixed-training pipeline that produced a smooth and stable convergence that showed few oscillations in performance.\n",
        "\n",
        "Our model performed better than his when playing against a random agent as the second player. His win rate was only 58%, lower than our 64.5%."
      ],
      "metadata": {
        "id": "6pcevX5EZ3zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Connect Four Code**"
      ],
      "metadata": {
        "id": "eU5Zk4MBx3Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We followed the same process from Tic-Tac-Toe to build our Connect Four game and model architecture. First, we coded the game itself and a `game_play()` function that allowed us to play it in real time. Both the game code and play cell are at the bottom of this section."
      ],
      "metadata": {
        "id": "rDWuMnQXOs1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After confirming our Connect Four game worked, it was time to implement our model. The majority of our code was identical to our Tic-Tac-Toe model because of how similar the games are. There were still a few changes, such as coding the tokens as \"R\" and \"Y\" instead of X and O. The board was also coded differently, since Connect Four is a 6 by 7 board instead of a 3 by 3 board. We also didn't need the `flatten_action` and `unflatten_action` functions anymore, because you can only choose a column when playing Connect Four, not a row and a column."
      ],
      "metadata": {
        "id": "iYoBq_8CTwKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the board to a flat list of numbers: X is 1, O is -1, empty is 0- copied from tic tac toe but made some adjustments\n",
        "\n",
        "def board_to_tensor(board):\n",
        "  flat = []\n",
        "\n",
        "  for row in board:\n",
        "    for cell in row:\n",
        "      if cell == \"R\": #for red\n",
        "        flat.append(1)\n",
        "      elif cell == \"Y\":\n",
        "        flat.append(-1)\n",
        "      else:\n",
        "        flat.append(0)\n",
        "  return torch.tensor(flat, dtype=torch.float32)\n",
        "\n",
        "# return a list of available positions- only c this time b/c you only choose a column\n",
        "def available_moves(board):\n",
        "  moves = []\n",
        "\n",
        "  for c in range(7):\n",
        "      if board[0][c] == \" \": #different too, bc you can play a column if the top slot is empty\n",
        "        moves.append(c)\n",
        "  return moves"
      ],
      "metadata": {
        "id": "ZAKKaNV2x7ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The major difference in our `check_win` function was that in Connect Four, you need four tokens in a line in order to win rather than three."
      ],
      "metadata": {
        "id": "kCoUDszBUzKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check for win\n",
        "def check_win(board, player_id):\n",
        "    ROWS = 6\n",
        "    COLS = 7\n",
        "\n",
        "    # horizontal\n",
        "    for r in range(ROWS):\n",
        "        for c in range(COLS - 3):\n",
        "            if all(board[r][c+i] == player_id for i in range(4)):\n",
        "                return True\n",
        "\n",
        "    # vertical\n",
        "    for r in range(ROWS - 3):\n",
        "        for c in range(COLS):\n",
        "            if all(board[r+i][c] == player_id for i in range(4)):\n",
        "                return True\n",
        "\n",
        "    # diagonal down\n",
        "    for r in range(ROWS - 3):\n",
        "        for c in range(COLS - 3):\n",
        "            if all(board[r+i][c+i] == player_id for i in range(4)):\n",
        "                return True\n",
        "\n",
        "    # diagonal up\n",
        "    for r in range(3, ROWS):\n",
        "        for c in range(COLS - 3):\n",
        "            if all(board[r-i][c+i] == player_id for i in range(4)):\n",
        "                return True\n",
        "\n",
        "    return False"
      ],
      "metadata": {
        "id": "z_wY78J-yEm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Connect Four neural network structure was also very similar to its Tic-Tac-Toe counterpart. The main differences were we had 100 nodes in our first hidden layer and 50 nodes in our second hidden layer, since Connect Four is a more complex game. We also had 7 outputs, because as stated before, you can only choose a column to make your move in Connect Four."
      ],
      "metadata": {
        "id": "oPYFNAZ2WIO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network with TWO hidden layers (added extra layer!)\n",
        "class ConnectFourNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(42, 100),  # First hidden layer: 100 nodes\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 50),  # Second hidden layer: 50 nodes\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 7)     # Output layer: 7 possible moves\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Initialize network and optimizer\n",
        "net = ConnectFourNet()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "wHrAo_0xyHEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining legal moves, rewards, and punishments were along the same lines as what we did for Tic-Tac-Toe. The only difference was that the agent was only choosing a column, not a row and a column, because that is how Connect Four is traditionally played on a vertical board."
      ],
      "metadata": {
        "id": "5qcvMj0mW24s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reinforcement learning computer move\n",
        "# to choose a move, the network output is turned into a probability distribution\n",
        "def rl_computer_move(board):\n",
        "  state = board_to_tensor(board)         # convert current board into a flat tensor\n",
        "  logits = net(state)              # get the network's output logits\n",
        "  logits = torch.clamp(logits, -20, 20)  # prevents very large/small values\n",
        "  probs = torch.softmax(logits, dim=0)     # convert logits into probabilities using softmax\n",
        "\n",
        "# create a mask to mark legal moves (1 for empty, 0 for taken)\n",
        "# the agent selects a legal move as a sample based on these prob\n",
        "  mask = torch.zeros(7) #only 7 columns\n",
        "  legal_col = available_moves(board)\n",
        "\n",
        "  if not legal_col:\n",
        "    return None\n",
        "\n",
        "  for c in legal_col:\n",
        "    mask[c] = 1\n",
        "\n",
        "  # apply mask (illegal moves have 0 prob)\n",
        "  probs = probs * mask\n",
        "  total = probs.sum()     # sum of legal move prob\n",
        "\n",
        "  # if sum is 0, pick random legal move\n",
        "  if total.item() == 0:\n",
        "    c = random.choice(legal_col)\n",
        "    probs = torch.zeros(7)\n",
        "    probs[c] = 1.0\n",
        "  else:\n",
        "    probs = probs / total  # otherwise, normalize prob so they sum to 1\n",
        "\n",
        "# choose action\n",
        "  dist = torch.distributions.Categorical(probs)   # creates a probability for choosing each move\n",
        "  action = dist.sample()               # picks one action from the distribution\n",
        "  column = action.item()       # turns the pytorch tensor into integer and converts it into a board position\n",
        "\n",
        "  #drops piece in column- copied from sophie's code\n",
        "  for row in range(5, -1, -1):\n",
        "    if board[row][column] == \" \":\n",
        "      board[row][column] = \"Y\"\n",
        "      break\n",
        "\n",
        "  return dist.log_prob(action)  # return the log-probability of the chosen move\n",
        "                  # if the agent wins, we reward moves with high log_prob\n",
        "                  # if it loses, we punish those moves"
      ],
      "metadata": {
        "id": "fjaOcXfTyKHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of initializing with self-play, we trained the Connect Four model against a random agant."
      ],
      "metadata": {
        "id": "HkdmMXqDW4je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# play one game and train\n",
        "def train_one_game():\n",
        "  board = [[\" \" for _ in range(7)] for _ in range(6)] #7 by 6\n",
        "  player1 = \"Human\" #random \"human\" opponant\n",
        "  player2 = \"RL_Agent\"\n",
        "  current_player = player1\n",
        "\n",
        "  log_probs = []   # store chosen moves of RL (for learning)\n",
        "  final_reward = 0    # final reward (+1 win, -1 lose, 0 tie)\n",
        "  move_count = 0\n",
        "  max_moves = 42  # Safety limit\n",
        "\n",
        "  while move_count < max_moves:\n",
        "# human move (random for simulation, could be input)\n",
        "    if current_player == player1:\n",
        "      empty_moves = available_moves(board)   # get list of empty spots\n",
        "      if not empty_moves:\n",
        "        final_reward = 0  # tie\n",
        "        break\n",
        "\n",
        "      # pick a ramdon empty spot\n",
        "      c = random.choice(empty_moves)\n",
        "      placed = False\n",
        "      for row in range(5, -1, -1):  # Start from bottom (row 5)\n",
        "        if board[row][c] == \" \":\n",
        "          board[row][c] = \"R\"\n",
        "          placed = True\n",
        "          break\n",
        "      if not placed:\n",
        "        continue\n",
        "      move_count += 1\n",
        "\n",
        "# Check if human won after their move\n",
        "      if check_win(board, \"R\"):\n",
        "        final_reward = -1  # RL loses\n",
        "        break\n",
        "\n",
        "    # Check tie after human move\n",
        "      if all(board[0][c] != \" \" for c in range(7)):\n",
        "        final_reward = 0\n",
        "        break\n",
        "\n",
        "\n",
        "    # RL agent move\n",
        "    else:\n",
        "      empty_moves = available_moves(board)\n",
        "      if not empty_moves:\n",
        "        final_reward = 0  # Tie\n",
        "        break\n",
        "\n",
        "      log_prob = rl_computer_move(board)  # agent picks a move\n",
        "      if log_prob is not None:\n",
        "        log_probs.append(log_prob)\n",
        "\n",
        "      move_count += 1\n",
        "\n",
        "    # check if anyone wins\n",
        "    if check_win(board, \"Y\"):\n",
        "      final_reward = +1   # RL wins\n",
        "      break\n",
        "\n",
        "    # check tie\n",
        "    if not available_moves(board):\n",
        "      final_reward = 0\n",
        "      break\n",
        "\n",
        "    # switch players\n",
        "    current_player = player2 if current_player == player1 else player1\n",
        "\n",
        "# train the RL agent (reinforce)\n",
        "  if log_probs:    # RL must make at least one move\n",
        "    loss = -torch.stack(log_probs).sum() * final_reward  # reinforce formula\n",
        "    optimizer.zero_grad()  # reset gradients\n",
        "    loss.backward()     # compute gradients\n",
        "    torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "    optimizer.step()     # update neural network\n",
        "\n",
        "  return final_reward\n",
        "\n",
        "# RL agent wins: reward = +1 (the agent successfully placed Os and made the human (or random opponent) lose)\n",
        "# RL agent loses: reward = -1 (the agent made bad moves, and the opponent (X) won)\n",
        "# Tie game: reward = 0 (neither player won; the board is full)"
      ],
      "metadata": {
        "id": "L0aRYt_ryM7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "zKkx3l1jyQbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the RL agent for multiple games\n",
        "set_seed(42)\n",
        "\n",
        "episodes = 50000\n",
        "win_count = 0\n",
        "loss_count = 0\n",
        "tie_count = 0\n",
        "\n",
        "for episode in range(episodes):\n",
        "    r = train_one_game()\n",
        "\n",
        "    if r == 1:\n",
        "        win_count += 1\n",
        "    elif r == -1:\n",
        "        loss_count += 1\n",
        "    else:\n",
        "        tie_count += 1\n",
        "\n",
        "    if episode % 500 == 0:\n",
        "        print(f\"Episode {episode}, last game reward: {r}\")\n",
        "        print(f\"  Stats so far - Wins: {win_count}, Losses: {loss_count}, Ties: {tie_count}\")\n",
        "\n",
        "print(f\"Final training stats - Wins: {win_count}, Losses: {loss_count}, Ties: {tie_count}\")"
      ],
      "metadata": {
        "id": "xCcZX2bEySeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "df4cab87-bc07-467e-9249-bdc78a3f4bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, last game reward: 1\n",
            "  Stats so far - Wins: 1, Losses: 0, Ties: 0\n",
            "Episode 500, last game reward: 1\n",
            "  Stats so far - Wins: 302, Losses: 199, Ties: 0\n",
            "Episode 1000, last game reward: 1\n",
            "  Stats so far - Wins: 734, Losses: 267, Ties: 0\n",
            "Episode 1500, last game reward: 1\n",
            "  Stats so far - Wins: 1180, Losses: 321, Ties: 0\n",
            "Episode 2000, last game reward: 1\n",
            "  Stats so far - Wins: 1610, Losses: 391, Ties: 0\n",
            "Episode 2500, last game reward: 1\n",
            "  Stats so far - Wins: 2047, Losses: 454, Ties: 0\n",
            "Episode 3000, last game reward: -1\n",
            "  Stats so far - Wins: 2490, Losses: 511, Ties: 0\n",
            "Episode 3500, last game reward: 1\n",
            "  Stats so far - Wins: 2929, Losses: 572, Ties: 0\n",
            "Episode 4000, last game reward: 1\n",
            "  Stats so far - Wins: 3371, Losses: 630, Ties: 0\n",
            "Episode 4500, last game reward: 1\n",
            "  Stats so far - Wins: 3815, Losses: 686, Ties: 0\n",
            "Episode 5000, last game reward: 1\n",
            "  Stats so far - Wins: 4247, Losses: 754, Ties: 0\n",
            "Episode 5500, last game reward: 1\n",
            "  Stats so far - Wins: 4684, Losses: 817, Ties: 0\n",
            "Episode 6000, last game reward: 1\n",
            "  Stats so far - Wins: 5112, Losses: 889, Ties: 0\n",
            "Episode 6500, last game reward: 1\n",
            "  Stats so far - Wins: 5553, Losses: 948, Ties: 0\n",
            "Episode 7000, last game reward: 1\n",
            "  Stats so far - Wins: 5992, Losses: 1009, Ties: 0\n",
            "Episode 7500, last game reward: 1\n",
            "  Stats so far - Wins: 6434, Losses: 1067, Ties: 0\n",
            "Episode 8000, last game reward: 1\n",
            "  Stats so far - Wins: 6877, Losses: 1124, Ties: 0\n",
            "Episode 8500, last game reward: 1\n",
            "  Stats so far - Wins: 7303, Losses: 1198, Ties: 0\n",
            "Episode 9000, last game reward: 1\n",
            "  Stats so far - Wins: 7749, Losses: 1252, Ties: 0\n",
            "Episode 9500, last game reward: 1\n",
            "  Stats so far - Wins: 8188, Losses: 1313, Ties: 0\n",
            "Episode 10000, last game reward: 1\n",
            "  Stats so far - Wins: 8626, Losses: 1375, Ties: 0\n",
            "Episode 10500, last game reward: 1\n",
            "  Stats so far - Wins: 9066, Losses: 1435, Ties: 0\n",
            "Episode 11000, last game reward: 1\n",
            "  Stats so far - Wins: 9495, Losses: 1506, Ties: 0\n",
            "Episode 11500, last game reward: 1\n",
            "  Stats so far - Wins: 9924, Losses: 1577, Ties: 0\n",
            "Episode 12000, last game reward: 1\n",
            "  Stats so far - Wins: 10358, Losses: 1643, Ties: 0\n",
            "Episode 12500, last game reward: 1\n",
            "  Stats so far - Wins: 10798, Losses: 1703, Ties: 0\n",
            "Episode 13000, last game reward: -1\n",
            "  Stats so far - Wins: 11224, Losses: 1777, Ties: 0\n",
            "Episode 13500, last game reward: 1\n",
            "  Stats so far - Wins: 11643, Losses: 1858, Ties: 0\n",
            "Episode 14000, last game reward: 1\n",
            "  Stats so far - Wins: 12079, Losses: 1922, Ties: 0\n",
            "Episode 14500, last game reward: 1\n",
            "  Stats so far - Wins: 12521, Losses: 1980, Ties: 0\n",
            "Episode 15000, last game reward: 1\n",
            "  Stats so far - Wins: 12960, Losses: 2041, Ties: 0\n",
            "Episode 15500, last game reward: 1\n",
            "  Stats so far - Wins: 13398, Losses: 2103, Ties: 0\n",
            "Episode 16000, last game reward: 1\n",
            "  Stats so far - Wins: 13846, Losses: 2155, Ties: 0\n",
            "Episode 16500, last game reward: 1\n",
            "  Stats so far - Wins: 14290, Losses: 2211, Ties: 0\n",
            "Episode 17000, last game reward: 1\n",
            "  Stats so far - Wins: 14728, Losses: 2273, Ties: 0\n",
            "Episode 17500, last game reward: 1\n",
            "  Stats so far - Wins: 15164, Losses: 2337, Ties: 0\n",
            "Episode 18000, last game reward: 1\n",
            "  Stats so far - Wins: 15594, Losses: 2407, Ties: 0\n",
            "Episode 18500, last game reward: 1\n",
            "  Stats so far - Wins: 16026, Losses: 2475, Ties: 0\n",
            "Episode 19000, last game reward: 1\n",
            "  Stats so far - Wins: 16459, Losses: 2542, Ties: 0\n",
            "Episode 19500, last game reward: 1\n",
            "  Stats so far - Wins: 16892, Losses: 2609, Ties: 0\n",
            "Episode 20000, last game reward: 1\n",
            "  Stats so far - Wins: 17340, Losses: 2661, Ties: 0\n",
            "Episode 20500, last game reward: 1\n",
            "  Stats so far - Wins: 17801, Losses: 2700, Ties: 0\n",
            "Episode 21000, last game reward: 1\n",
            "  Stats so far - Wins: 18234, Losses: 2767, Ties: 0\n",
            "Episode 21500, last game reward: 1\n",
            "  Stats so far - Wins: 18672, Losses: 2829, Ties: 0\n",
            "Episode 22000, last game reward: -1\n",
            "  Stats so far - Wins: 19103, Losses: 2898, Ties: 0\n",
            "Episode 22500, last game reward: -1\n",
            "  Stats so far - Wins: 19542, Losses: 2959, Ties: 0\n",
            "Episode 23000, last game reward: 1\n",
            "  Stats so far - Wins: 19975, Losses: 3026, Ties: 0\n",
            "Episode 23500, last game reward: -1\n",
            "  Stats so far - Wins: 20411, Losses: 3090, Ties: 0\n",
            "Episode 24000, last game reward: 1\n",
            "  Stats so far - Wins: 20856, Losses: 3145, Ties: 0\n",
            "Episode 24500, last game reward: 1\n",
            "  Stats so far - Wins: 21284, Losses: 3217, Ties: 0\n",
            "Episode 25000, last game reward: 1\n",
            "  Stats so far - Wins: 21720, Losses: 3281, Ties: 0\n",
            "Episode 25500, last game reward: 1\n",
            "  Stats so far - Wins: 22160, Losses: 3341, Ties: 0\n",
            "Episode 26000, last game reward: 1\n",
            "  Stats so far - Wins: 22591, Losses: 3410, Ties: 0\n",
            "Episode 26500, last game reward: 1\n",
            "  Stats so far - Wins: 23023, Losses: 3478, Ties: 0\n",
            "Episode 27000, last game reward: 1\n",
            "  Stats so far - Wins: 23462, Losses: 3539, Ties: 0\n",
            "Episode 27500, last game reward: 1\n",
            "  Stats so far - Wins: 23904, Losses: 3597, Ties: 0\n",
            "Episode 28000, last game reward: 1\n",
            "  Stats so far - Wins: 24331, Losses: 3670, Ties: 0\n",
            "Episode 28500, last game reward: 1\n",
            "  Stats so far - Wins: 24784, Losses: 3717, Ties: 0\n",
            "Episode 29000, last game reward: 1\n",
            "  Stats so far - Wins: 25217, Losses: 3784, Ties: 0\n",
            "Episode 29500, last game reward: 1\n",
            "  Stats so far - Wins: 25649, Losses: 3852, Ties: 0\n",
            "Episode 30000, last game reward: 1\n",
            "  Stats so far - Wins: 26087, Losses: 3914, Ties: 0\n",
            "Episode 30500, last game reward: 1\n",
            "  Stats so far - Wins: 26515, Losses: 3986, Ties: 0\n",
            "Episode 31000, last game reward: 1\n",
            "  Stats so far - Wins: 26940, Losses: 4061, Ties: 0\n",
            "Episode 31500, last game reward: 1\n",
            "  Stats so far - Wins: 27378, Losses: 4123, Ties: 0\n",
            "Episode 32000, last game reward: 1\n",
            "  Stats so far - Wins: 27825, Losses: 4176, Ties: 0\n",
            "Episode 32500, last game reward: 1\n",
            "  Stats so far - Wins: 28263, Losses: 4238, Ties: 0\n",
            "Episode 33000, last game reward: 1\n",
            "  Stats so far - Wins: 28709, Losses: 4292, Ties: 0\n",
            "Episode 33500, last game reward: 1\n",
            "  Stats so far - Wins: 29142, Losses: 4359, Ties: 0\n",
            "Episode 34000, last game reward: 1\n",
            "  Stats so far - Wins: 29570, Losses: 4431, Ties: 0\n",
            "Episode 34500, last game reward: 1\n",
            "  Stats so far - Wins: 30009, Losses: 4492, Ties: 0\n",
            "Episode 35000, last game reward: 1\n",
            "  Stats so far - Wins: 30445, Losses: 4556, Ties: 0\n",
            "Episode 35500, last game reward: 1\n",
            "  Stats so far - Wins: 30869, Losses: 4632, Ties: 0\n",
            "Episode 36000, last game reward: 1\n",
            "  Stats so far - Wins: 31322, Losses: 4679, Ties: 0\n",
            "Episode 36500, last game reward: 1\n",
            "  Stats so far - Wins: 31779, Losses: 4722, Ties: 0\n",
            "Episode 37000, last game reward: 1\n",
            "  Stats so far - Wins: 32220, Losses: 4781, Ties: 0\n",
            "Episode 37500, last game reward: 1\n",
            "  Stats so far - Wins: 32657, Losses: 4844, Ties: 0\n",
            "Episode 38000, last game reward: 1\n",
            "  Stats so far - Wins: 33087, Losses: 4914, Ties: 0\n",
            "Episode 38500, last game reward: 1\n",
            "  Stats so far - Wins: 33516, Losses: 4985, Ties: 0\n",
            "Episode 39000, last game reward: 1\n",
            "  Stats so far - Wins: 33960, Losses: 5041, Ties: 0\n",
            "Episode 39500, last game reward: 1\n",
            "  Stats so far - Wins: 34408, Losses: 5093, Ties: 0\n",
            "Episode 40000, last game reward: 1\n",
            "  Stats so far - Wins: 34851, Losses: 5150, Ties: 0\n",
            "Episode 40500, last game reward: 1\n",
            "  Stats so far - Wins: 35289, Losses: 5212, Ties: 0\n",
            "Episode 41000, last game reward: 1\n",
            "  Stats so far - Wins: 35727, Losses: 5274, Ties: 0\n",
            "Episode 41500, last game reward: 1\n",
            "  Stats so far - Wins: 36169, Losses: 5332, Ties: 0\n",
            "Episode 42000, last game reward: 1\n",
            "  Stats so far - Wins: 36609, Losses: 5392, Ties: 0\n",
            "Episode 42500, last game reward: 1\n",
            "  Stats so far - Wins: 37053, Losses: 5448, Ties: 0\n",
            "Episode 43000, last game reward: 1\n",
            "  Stats so far - Wins: 37495, Losses: 5506, Ties: 0\n",
            "Episode 43500, last game reward: 1\n",
            "  Stats so far - Wins: 37932, Losses: 5569, Ties: 0\n",
            "Episode 44000, last game reward: 1\n",
            "  Stats so far - Wins: 38357, Losses: 5644, Ties: 0\n",
            "Episode 44500, last game reward: 1\n",
            "  Stats so far - Wins: 38804, Losses: 5697, Ties: 0\n",
            "Episode 45000, last game reward: 1\n",
            "  Stats so far - Wins: 39252, Losses: 5749, Ties: 0\n",
            "Episode 45500, last game reward: 1\n",
            "  Stats so far - Wins: 39705, Losses: 5796, Ties: 0\n",
            "Episode 46000, last game reward: 1\n",
            "  Stats so far - Wins: 40135, Losses: 5866, Ties: 0\n",
            "Episode 46500, last game reward: 1\n",
            "  Stats so far - Wins: 40577, Losses: 5924, Ties: 0\n",
            "Episode 47000, last game reward: 1\n",
            "  Stats so far - Wins: 41013, Losses: 5988, Ties: 0\n",
            "Episode 47500, last game reward: 1\n",
            "  Stats so far - Wins: 41440, Losses: 6061, Ties: 0\n",
            "Episode 48000, last game reward: 1\n",
            "  Stats so far - Wins: 41882, Losses: 6119, Ties: 0\n",
            "Episode 48500, last game reward: -1\n",
            "  Stats so far - Wins: 42314, Losses: 6187, Ties: 0\n",
            "Episode 49000, last game reward: -1\n",
            "  Stats so far - Wins: 42752, Losses: 6249, Ties: 0\n",
            "Episode 49500, last game reward: 1\n",
            "  Stats so far - Wins: 43187, Losses: 6314, Ties: 0\n",
            "Model saved as connect4_checkpoint_v1.pth\n",
            "Final training stats - Wins: 43615, Losses: 6385, Ties: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluating Our Connect Four Model\n",
        "\n",
        "We had our Connect Four model play 10,000 games instead of 1,000 for evaluation because of the game's increased complexity. Its opponent was a random agent. The results are plotted in the bar graph below."
      ],
      "metadata": {
        "id": "8kvDUtT3_8Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Assessment - How likely the computer will win the game\n",
        "\n",
        "def evaluate_win_rate(num_games=10000):\n",
        "    wins = 0\n",
        "    ties = 0\n",
        "    losses = 0\n",
        "\n",
        "    for _ in range(num_games):\n",
        "\n",
        "        # Initialize empty board\n",
        "        board = [[\" \" for _ in range(7)] for _ in range(6)] #7 by 6\n",
        "        current_player = \"Human\"  # Random human opponent\n",
        "\n",
        "        while True:\n",
        "            if current_player == \"Human\":\n",
        "                empty_moves = available_moves(board)\n",
        "                if not empty_moves:\n",
        "                    # Tie\n",
        "                    ties += 1\n",
        "                    break\n",
        "                c = random.choice(empty_moves)\n",
        "                for row in reversed(range(6)):\n",
        "                    if board[row][c] == \" \":\n",
        "                        board[row][c] = \"R\"\n",
        "                        break\n",
        "            else:\n",
        "                rl_computer_move(board)\n",
        "\n",
        "            # Check win\n",
        "            if check_win(board, \"R\"):\n",
        "                losses += 1\n",
        "                break\n",
        "            elif check_win(board, \"Y\"):\n",
        "                wins += 1\n",
        "                break\n",
        "\n",
        "            # Check tie\n",
        "            if all(board[0][c] != \" \" for c in range(7)):\n",
        "              ties += 1\n",
        "              break\n",
        "\n",
        "            # Switch player\n",
        "            current_player = \"RL_Agent\" if current_player == \"Human\" else \"Human\"\n",
        "\n",
        "\n",
        "    # Compute rates\n",
        "    results = {\n",
        "        \"win\": wins / num_games,\n",
        "        \"loss\": losses / num_games,\n",
        "        \"tie\": ties / num_games\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Number of Games: {num_games}, Wins={wins}, Losses={losses}, Ties={ties}\")\n",
        "    print(f\"Win rate: {results['win']:.2%}, Loss rate: {results['loss']:.2%}, Tie rate: {results['tie']:.2%}\")\n",
        "\n",
        "    # Plot bar chart\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(results.keys(), results.values(), color=[\"green\", \"red\", \"gray\"])\n",
        "    plt.ylabel(\"Rate\")\n",
        "    plt.title(f\"RL Agent Performance over {num_games} Games\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "x__bKxqHyU-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.load_state_dict(torch.load(\"connect4_checkpoint_v1.pth\"))\n",
        "net.eval()\n",
        "\n",
        "set_seed(42)\n",
        "evaluate_win_rate(num_games=50000)"
      ],
      "metadata": {
        "id": "Y4iLWNG0yZKI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "76360c1d-c97b-4c83-fdbe-920831d223ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Games: 50000, Wins=44146, Losses=5852, Ties=2\n",
            "Win rate: 88.29%, Loss rate: 11.70%, Tie rate: 0.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'win': 0.88292, 'loss': 0.11704, 'tie': 4e-05}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the Connect Four code and the cell to initiate real-time play."
      ],
      "metadata": {
        "id": "omsvSC14Xh5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ansi colors\n",
        "reset = \"\\033[0m\"\n",
        "red = \"\\033[31m\"\n",
        "yellow = \"\\033[33m\"\n",
        "\n",
        "#pieces\n",
        "piece = \"●\"\n",
        "\n",
        "#identifiers that we stored in the board\n",
        "empty = \" \"\n",
        "red_id   = \"R\"\n",
        "yellow_id = \"Y\"\n",
        "\n",
        "# width of each printed cell -- it was looking funky without explicitly stating cell width\n",
        "cell_width = 3\n",
        "\n",
        "def display_for(cell):\n",
        "    if cell == red_id:\n",
        "        return f\"{red}{piece}{reset}\"\n",
        "    elif cell == yellow_id:\n",
        "        return f\"{yellow}{piece}{reset}\"\n",
        "    else:\n",
        "        return \" \"\n",
        "\n",
        "def game_board(board):\n",
        "\n",
        "    cols = len(board[0])\n",
        "    total_width = cols * (cell_width + 1) + 1  #for borders\n",
        "\n",
        "    print(\"-\" * total_width)\n",
        "    for row in board:\n",
        "        print(\"|\", end=\"\")\n",
        "        for cell in row:\n",
        "            left = (cell_width - 1) // 2\n",
        "            right = cell_width - 1 - left\n",
        "\n",
        "            vis = display_for(cell)\n",
        "            print(\" \" * left + vis + \" \" * right + \"|\", end=\"\")\n",
        "        print(\"\\n\" + \"-\" * total_width)\n",
        "\n",
        "    # print column numbers centered in the same width\n",
        "    print(\" \", end=\"\")\n",
        "    for c in range(cols):\n",
        "        print(f\"{str(c+1):^{cell_width}} \", end=\"\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# drops a piece into a column bottom-up\n",
        "def drop_piece(board, col, piece_id):\n",
        "    for row in range(5, -1, -1):  # bottom up\n",
        "        if board[row][col] == empty:\n",
        "            board[row][col] = piece_id\n",
        "            return True\n",
        "    return False  # if the column is full\n",
        "\n",
        "# checks for win\n",
        "def check_win(board, player_id):\n",
        "    ROWS = 6\n",
        "    COLS = 7\n",
        "\n",
        "    # horizontal\n",
        "    for r in range(ROWS):\n",
        "        for c in range(COLS - 3):\n",
        "            if all(board[r][c+i] == player_id for i in range(4)):\n",
        "                return True\n",
        "\n",
        "    # vertical\n",
        "    for r in range(ROWS - 3):\n",
        "        for c in range(COLS):\n",
        "            if all(board[r+i][c] == player_id for i in range(4)):\n",
        "                return True\n",
        "\n",
        "    # diagonal down\n",
        "    for r in range(ROWS - 3):\n",
        "        for c in range(COLS - 3):\n",
        "            if all(board[r+i][c+i] == player_id for i in range(4)):\n",
        "                return True\n",
        "\n",
        "    # diagonal up\n",
        "    for r in range(3, ROWS):\n",
        "        for c in range(COLS - 3):\n",
        "            if all(board[r-i][c+i] == player_id for i in range(4)):\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# computer move\n",
        "def computer_move(board):\n",
        "    valid_cols = [c for c in range(7) if board[0][c] == empty]\n",
        "    if valid_cols:\n",
        "        col = random.choice(valid_cols)\n",
        "        drop_piece(board, col, yellow_id)\n",
        "\n",
        "def game_play():\n",
        "    board = [[empty for _ in range(7)] for _ in range(6)]\n",
        "\n",
        "    player1 = \"Sophie (the queen)\"\n",
        "    player2 = \"Putie\"\n",
        "\n",
        "    current_player = player1\n",
        "\n",
        "    while True:\n",
        "        game_board(board)\n",
        "\n",
        "        if current_player == player1:\n",
        "            while True:\n",
        "                try:\n",
        "                    col = int(input(\"Pick a column (1-7): \")) - 1\n",
        "                except ValueError:\n",
        "                    print(\"You tricky bastard that's not a number.\")\n",
        "                    continue\n",
        "\n",
        "                if col not in range(7):\n",
        "                    print(\"That column doesn't exist. Enter 1–7 please.\")\n",
        "                    continue\n",
        "\n",
        "                if drop_piece(board, col, red_id):\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"That column is full :( try again.\")\n",
        "        else:\n",
        "            computer_move(board)\n",
        "\n",
        "        # check win\n",
        "        player_id = red_id if current_player == player1 else yellow_id\n",
        "        if check_win(board, player_id):\n",
        "            game_board(board)\n",
        "            print(f\"{current_player} wins!\")\n",
        "            break\n",
        "\n",
        "        # tie\n",
        "        if all(board[0][c] != empty for c in range(7)):\n",
        "            game_board(board)\n",
        "            print(\"It's a tie!\")\n",
        "            break\n",
        "\n",
        "        # swap turns\n",
        "        current_player = player2 if current_player == player1 else player1"
      ],
      "metadata": {
        "id": "fIF2pbAXya84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game_play()"
      ],
      "metadata": {
        "id": "iykITX_wXRmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b5776106-431a-49b3-b323-195ff57077db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "  1   2   3   4   5   6   7  \n",
            "\n",
            "Pick a column (1-7): 3\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   | \u001b[31m●\u001b[0m |   |   |   |   |\n",
            "-----------------------------\n",
            "  1   2   3   4   5   6   7  \n",
            "\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   | \u001b[31m●\u001b[0m |   |   | \u001b[33m●\u001b[0m |   |\n",
            "-----------------------------\n",
            "  1   2   3   4   5   6   7  \n",
            "\n",
            "Pick a column (1-7): 3\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   | \u001b[31m●\u001b[0m |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   | \u001b[31m●\u001b[0m |   |   | \u001b[33m●\u001b[0m |   |\n",
            "-----------------------------\n",
            "  1   2   3   4   5   6   7  \n",
            "\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   | \u001b[31m●\u001b[0m |   |   |   |   |\n",
            "-----------------------------\n",
            "| \u001b[33m●\u001b[0m |   | \u001b[31m●\u001b[0m |   |   | \u001b[33m●\u001b[0m |   |\n",
            "-----------------------------\n",
            "  1   2   3   4   5   6   7  \n",
            "\n",
            "Pick a column (1-7): 3\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   | \u001b[31m●\u001b[0m |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   | \u001b[31m●\u001b[0m |   |   |   |   |\n",
            "-----------------------------\n",
            "| \u001b[33m●\u001b[0m |   | \u001b[31m●\u001b[0m |   |   | \u001b[33m●\u001b[0m |   |\n",
            "-----------------------------\n",
            "  1   2   3   4   5   6   7  \n",
            "\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   | \u001b[31m●\u001b[0m |   |   |   |   |\n",
            "-----------------------------\n",
            "| \u001b[33m●\u001b[0m |   | \u001b[31m●\u001b[0m |   |   |   |   |\n",
            "-----------------------------\n",
            "| \u001b[33m●\u001b[0m |   | \u001b[31m●\u001b[0m |   |   | \u001b[33m●\u001b[0m |   |\n",
            "-----------------------------\n",
            "  1   2   3   4   5   6   7  \n",
            "\n",
            "Pick a column (1-7): 3\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   |   |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   | \u001b[31m●\u001b[0m |   |   |   |   |\n",
            "-----------------------------\n",
            "|   |   | \u001b[31m●\u001b[0m |   |   |   |   |\n",
            "-----------------------------\n",
            "| \u001b[33m●\u001b[0m |   | \u001b[31m●\u001b[0m |   |   |   |   |\n",
            "-----------------------------\n",
            "| \u001b[33m●\u001b[0m |   | \u001b[31m●\u001b[0m |   |   | \u001b[33m●\u001b[0m |   |\n",
            "-----------------------------\n",
            "  1   2   3   4   5   6   7  \n",
            "\n",
            "Sophie (the queen) wins!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Challenges Encountered**:\n",
        "\n",
        "**Network Architecture Tuning**:\n",
        "\n",
        "Initial experiments with a single-layer network produced poor results:\n",
        "\n",
        "* Challenge: Single hidden layer (42 -> 64 -> 7) achieved only ~60% win rate against random opponent, indicating insufficient ability to learn complex chessboard patterns.\n",
        "\n",
        "* Solution: Expanded to two hidden layers (42 -> 100 -> 50 -> 7)\n",
        "\n",
        "* Result: Win rate improved to ~88%, validating the need for deeper architecture\n",
        "\n",
        "* Trade-off: More parameters increased training time but improved performance\n",
        "\n",
        "\n",
        "**Insufficient Rewards**:\n",
        "\n",
        "The Reinforce algorithm only provides feedback at game end:\n",
        "\n",
        "* Challenge: A game might last 20-42 moves, but only the final outcome (+1, -1, or 0) guides learning. So is difficult for the agent to determine which specific moves were good or bad. For example, If the agent loses after 30 moves, should move 5, move 15, or move 28 be punished? So the current limitation is that the algorithm rewards/punished all moves equally based on final outcome. This contributes to slower learning and difficulty discovering subtle strategic patterns.\n",
        "\n",
        "* Possible solutions: We considered adding intermediate rewards for \"three-hit combos\" or blocking the opponent, but we abandoned this approach to avoid affecting the learned strategies.\n",
        "\n",
        "**Findings**:\n",
        "The most significant finding is the performance plateau at ~88% win rate. Extended training from 10,000 to 100,000 episodes failed to improve performance, this may suggesting that:\n",
        "\n",
        "1. Opponent Limitation: The random opponent creates a skill ceiling. The agent cannot learn beyond what random play teaches, as it never encounters multi-move traps or complex defensive patterns.\n",
        "\n",
        "2. Architectural Constraints: The fully connected network architecture may not capture spatial relationships between board positions and temporal dependencies across moves. Convolutional neural networks (CNNs) might better recognize spatial patterns like diagonal threats or board control."
      ],
      "metadata": {
        "id": "4tzJyH4ZgFgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discussion and Explanation of Analysis**\n",
        "\n",
        "**Training Performance**\n",
        "\n",
        "Over 100,000 training episodes, the agent demonstrated substantial learning:\n",
        "\n",
        "* Wins: 44,146 (88.29%)\n",
        "* Losses: 5,852 (11.70%)\n",
        "* Ties: 2 (0.004%)\n",
        "\n",
        "**Learning Progression**:\n",
        "\n",
        "* Episodes 0-500: 88.2% win rate\n",
        "* Episodes 10,000-10,500: 87.2% win rate\n",
        "* Episodes 99,500-100,000: 88.29% win rate\n",
        "\n",
        "The consistently high win rate throughout training (approximately 87-89%) shows that the agent quickly learned effective strategies against the random opponent.\n",
        "\n",
        "**Evaluation Performance:**\n",
        "\n",
        "Post-training evaluation over 10,000 independent games yielded:\n",
        "\n",
        "* Win rate: 88.29%\n",
        "* Loss rate: 11.70%\n",
        "* Tie rate: 0.01%\n",
        "\n",
        "The tie rate (0.01% in evaluation) suggests that games typically reach decisive conclusions, with either the agent or opponent establishing winning connections.\n",
        "\n",
        "**Strategic Behavior Analysis**\n",
        "\n",
        "Based on the training results, the agent likely learned several fundamental Connect 4 strategies:\n",
        "\n",
        "* High win rate suggests effective identification of winning moves.\n",
        "* The failure rate of approximately 12% indicates that there are occasional instances where blocking the opponent fails.\n",
        "* Easiest winning pattern to learn.\n",
        "\n",
        "**Comparison of Training vs. Evaluation Performance**\n",
        "The training (win rate 89.34%) and evaluation (win rate 87.90%) performances are almost identical, indicating that the model does not overfit specific game sequences. The learned policy is stable and reproducible. And with 100,000+ games, the ~1.5% difference is likely within normal variance."
      ],
      "metadata": {
        "id": "LBb4mq6XfVX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Comparison with Literature**\n",
        "For Connect Four, our results most closely resemble the trends reported in the article Connect Four: Reinforcement Learning Approach by Brandon Leung (2018). In Leung’s experiments, win-rates against a random agent initially fluctuated heavily due to sparse rewards and the larger action space, then slowly climbed as the agent learned basic blocking and vertical strategies. Our results show the same pattern: long periods of noisy improvement followed by slow stabilization. Like Leung’s model, our agent struggled to reach consistently dominant play without additional heuristics. Our models had similar win rates, where his fluctuated between 80% and 95%, compared to our 88%.\n",
        "\n",
        "And finally, our research brought us to a GitHub community project by Neo Yung that also contained an implementation of a reinforcement learning agent trained to play Connect Four. This work shows a much sharper improvement curve as Yung incorporates stronger heuristics, deeper networks, and more structured reward shaping. Compared to that system, our performance curve also follows the same qualitative trajectory—early instability, mid-training improvement, and late-training plateau. Our agent was able to achieve a win rate (88%) close to Yung's (about 92%) although Yung had trained for 20,000 games while our model was only trained for 10,000 games. Thus, our results in both Tic-Tac-Toe and Connect Four show clear parallels to findings reported in earlier DRL game-learning studies."
      ],
      "metadata": {
        "id": "-HqyA3kdaIO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion:**\n",
        "\n",
        "Across both Tic-Tac-Toe and Connect Four, this project demonstrates how deep reinforcement learning can acquire increasingly strategic behavior through structured self-play, iterative refinement, and careful architectural design. Starting with Tic-Tac-Toe allowed us to diagnose core DRL challenges—particularly perspective imbalance and sparse reward signals—before scaling to the more complex environment of Connect Four. Through a multi-phase training curriculum and deeper network design, the Tic-Tac-Toe agent ultimately achieved perfect defensive play, surpassing the performance reported  in simpler Q-learning implementations such as in the article by Kaneel Senevirathne (2021). His implementation did not reach fully symmetric, optimal strategies. Similarly, our Connect Four results, while strong against random opponents, align with findings from the article by Brandon Leung (2018) which also shows performance plateaus when using feed-forward networks and random-opponent training.\n",
        "\n",
        "These comparisons reinforce a consistent pattern in the literature: DRL agents can learn effective tactics in structured board games, but architectural choices, opponent quality, and training regimen strongly determine whether agents progress beyond basic competence. Future progress will likely require more expressive architectures, stronger adversarial training, and richer reward structures. Overall, this project not only mirrors key trends reported in prior work but also highlights practical insights into how DRL agents learn, where they stall, and how training pipelines can be improved as strategic complexity increases."
      ],
      "metadata": {
        "id": "iFTWYC_kukHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sources**:"
      ],
      "metadata": {
        "id": "DTsBcBhwNaIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Reinforcement Learning (2021, September 21). | PNNL. (n.d.). Www.pnnl.gov.  \n",
        "  https://www.pnnl.gov/explainer-articles/deep-reinforcement-learning\n",
        "\n",
        "Fulton, S. (2025, June 11). Brighten Up Your Terminal with ANSI Codes. Medium.\n",
        "https://medium.com/@fulton_shaun/brighten-up-your-terminal-with-ansi-codes-cb5f5d2ff085\n",
        "\n",
        "GeeksforGeeks. (2020, May 5). Python random.choices() method. GeeksforGeeks.\n",
        "https://www.geeksforgeeks.org/python/random-choices-method-in-python/\n",
        "\n",
        "Kaili, G., & Liu, M. (n.d.). (2023, February 28). Perspectives on the Social Impacts of\n",
        "Reinforcement Learning with Human Feedback. https://arxiv.org/pdf/2303.02891\n",
        "\n",
        "Kaneel Senevirathne. (2021, August 12). Teaching agents to play tic-tac-toe using Reinforcement Learning. Medium. http://medium.com/@kaneel.senevirathne/teaching-agents-to-play-tic-tac-toe-using-reinforcement-learning-7a9d4d6ee9b3\n",
        "\n",
        "\n",
        "Leung, B. (2020). Creating a Reinforcement Learning AI to play Connect Four. https://b7leung.github.io/files/Connect%20Four.pdf\n",
        "\n",
        "\n",
        "Maynez, A. (2021, March 18). Deep Q Learning for Tic Tac Toe. The\n",
        "Minimum Viable Model. https://the-mvm.github.io/deep-q-learning-tic-tac-toe.html\n",
        "\n",
        "Merchán, E. C. G. (2023, October 18). Why deep reinforcement learning is going to be the next\n",
        "big deal in AI. Medium. https://medium.com/@eduardogarrido90/why-deep-reinforcement-learning-is-going-to-be-the-next-big-deal-in-ai-2e796bdf47d2\n",
        "\n",
        "Mikulski, B. (2019, July 19). Bellman Equation Explained: The Foundation of Reinforcement Learning Algorithms. https://mikulskibartosz.name/bellman-equation-explained\n",
        "\n",
        "\n",
        "Neo Yung. (2025). A reinforcement learning agent trained without prior human knowledge. GitHub. https://github.com/neoyung/connect-4\n",
        "\n",
        "\n",
        "Raveaux, R. (2025). Reinforcement Learning by QLearning - Connect Four. Free.fr. http://romain.raveaux.free.fr/document/ReinforcementLearningbyQLearningConnectFourGame.html\n",
        "\n",
        "Tic-Tac-Toe with a Neural Network. (2019, December 27). Nestedsoftware.com. https://nestedsoftware.com/2019/12/27/tic-tac-toe-with-a-neural-network-1fjn.206436.html\n",
        "\n",
        "Whittlestone, J., Arulkumaran, K., & Crosby, M. (2021, March 8). The Societal Implications of\n",
        "Deep Reinforcement Learning. Journal of Artificial Intelligence Research, 70. https://doi.org/10.1613/jair.1.12360\n",
        "\n",
        "\n",
        "Yadav, A. (2024, September 20). Tic-Tac-Toe Reinforcement Learning Project. Medium; Biased-Algorithms.\n",
        "https://medium.com/biased-algorithms/tic-tac-toe-reinforcement-learning-project-23f015711f0d\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9auU-lty0OhF"
      }
    }
  ]
}